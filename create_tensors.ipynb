{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_to_2d(data):\n",
    "    shape = data.shape\n",
    "    print(shape)\n",
    "    vector_len = np.prod(shape[:-1])\n",
    "    time_steps = shape[-1]\n",
    "    flat_data = data.reshape(vector_len, time_steps)\n",
    "    flat_data = np.transpose(flat_data)\n",
    "    #print(transposed_flat_data.shape)\n",
    "    print(flat_data.shape)\n",
    "    return flat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(flat_data):\n",
    "    pca = PCA(n_components=200)\n",
    "    pca_data = pca.fit_transform(flat_data)\n",
    "    #print(pca_data.shape)\n",
    "    expl_var = sum(pca.explained_variance_ratio_)\n",
    "    return pca_data, expl_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_unzipped_files(root_dir):\n",
    "    gz_files = glob.glob(os.path.join(root_dir, '**', '*.gz'), recursive=True)\n",
    "    en_subj_data = []\n",
    "    ch_subj_data = []\n",
    "    en_fcount = 0\n",
    "    ch_fcount = 0\n",
    "    vars = []\n",
    "    for gz_file in tqdm(gz_files):\n",
    "        try:\n",
    "            img = nib.load(gz_file)\n",
    "            data = img.get_fdata()\n",
    "            data = flatten_to_2d(data)\n",
    "            data, var = pca(data)\n",
    "            vars.append(var)\n",
    "            fields = gz_file.split('-')\n",
    "            if 'EN' in fields[1]:\n",
    "                sub = fields[1]\n",
    "                en_subj_data.append(data)\n",
    "                en_fcount += 1\n",
    "            else:\n",
    "                ch_subj_data.append(data)\n",
    "                ch_fcount += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {gz_file}: {e}\")\n",
    "    return en_subj_data, ch_subj_data, vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(54679) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "  0%|          | 0/657 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 90, 74, 322)\n",
      "(322, 486180)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/657 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m root_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLittlePrince\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m en_subj_data, ch_subj_data, \u001b[38;5;28mvars\u001b[39m \u001b[38;5;241m=\u001b[39m process_unzipped_files(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_directory, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mderivatives\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[69], line 18\u001b[0m, in \u001b[0;36mprocess_unzipped_files\u001b[0;34m(root_dir)\u001b[0m\n\u001b[1;32m     16\u001b[0m data \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mget_fdata()\n\u001b[1;32m     17\u001b[0m data \u001b[38;5;241m=\u001b[39m flatten_to_2d(data)\n\u001b[0;32m---> 18\u001b[0m data, var \u001b[38;5;241m=\u001b[39m pca(data)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mvars\u001b[39m\u001b[38;5;241m.\u001b[39mappend(var)\n\u001b[1;32m     20\u001b[0m fields \u001b[38;5;241m=\u001b[39m gz_file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[68], line 3\u001b[0m, in \u001b[0;36mpca\u001b[0;34m(flat_data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpca\u001b[39m(flat_data):\n\u001b[1;32m      2\u001b[0m     pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     pca_data \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(flat_data)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#print(pca_data.shape)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     expl_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(pca\u001b[38;5;241m.\u001b[39mexplained_variance_ratio_)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:454\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    433\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[1;32m    455\u001b[0m     U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:516\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_full(X, n_components)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_truncated(X, n_components, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:642\u001b[0m, in \u001b[0;36mPCA._fit_truncated\u001b[0;34m(self, X, n_components, svd_solver)\u001b[0m\n\u001b[1;32m    640\u001b[0m     X \u001b[38;5;241m=\u001b[39m _implicit_column_offset(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_)\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 642\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmean(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    643\u001b[0m     X \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3464\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3461\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_mean(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   3465\u001b[0m                       out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/_methods.py:181\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    178\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    179\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m ret \u001b[38;5;241m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "root_directory = 'LittlePrince'\n",
    "en_subj_data, ch_subj_data, vars = process_unzipped_files(os.path.join(root_directory, 'derivatives'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in all the data takes several hours, so save it to a zip file of numpy arrays\n",
    "ch_file_path = 'ch_runs.npz'  \n",
    "en_file_path = 'en_runs.npz'\n",
    "\n",
    "np.savez(ch_file_path, *ch_subj_data)\n",
    "np.savez(en_file_path, *ch_subj_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "360\n",
      "(322, 200)\n",
      "(343, 200)\n"
     ]
    }
   ],
   "source": [
    "ch = np.load('ch_runs.npz')\n",
    "en = np.load('en_runs.npz')\n",
    "\n",
    "print(len(ch))\n",
    "print(len(en))\n",
    "\n",
    "ch_subj_data = [ch[i] for i in ch]\n",
    "en_subj_data = [en[i] for i in en]\n",
    "print(ch_subj_data[0].shape)\n",
    "print(en_subj_data[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 2816, 200)\n",
      "(33, 2977, 200)\n",
      "(35, 2816, 200)\n"
     ]
    }
   ],
   "source": [
    "# Create stacks of 9 runs to store data by subject\n",
    "# There are 9 runs per subject\n",
    "\n",
    "en_num_subjs = int(len(en_subj_data) / 9)\n",
    "en_stacked = np.zeros((en_num_subjs, 2816, 200)) # 2816 is number of time points imaged for EN\n",
    "\n",
    "for i in range(en_num_subjs):\n",
    "  loc = i * 9\n",
    "  subj_data = np.vstack((en_subj_data[loc:loc+9]))\n",
    "  en_stacked[i, :, :] = subj_data\n",
    "\n",
    "# Repeat for Chinese\n",
    "\n",
    "ch_num_subjs = int(len(ch_subj_data) / 9)\n",
    "ch_stacked = np.zeros((ch_num_subjs, 2977, 200)) # 2977 is number of time points imaged for CH\n",
    "\n",
    "for i in range(ch_num_subjs):\n",
    "  loc = i * 9\n",
    "  subj_data = np.vstack((ch_subj_data[loc:loc+9]))\n",
    "  ch_stacked[i, :, :] = subj_data\n",
    "\n",
    "print(en_stacked.shape)\n",
    "print(ch_stacked.shape)\n",
    "\n",
    "# To ensure approximately same number of images for each langauge, select only 35 of 40 EN participants\n",
    "\n",
    "en_stacked = en_stacked[:35, :, :]\n",
    "print(en_stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2816, 200)\n",
      "(3, 2816, 200)\n"
     ]
    }
   ],
   "source": [
    "# Create train, dev, and test splits by participant\n",
    "\n",
    "en_train_pps = en_stacked[:27,:,:]\n",
    "en_dev_pps = en_stacked[27:30,:,:]\n",
    "en_test_pps = en_stacked[30:,:,:]\n",
    "\n",
    "\n",
    "ch_train_pps = ch_stacked[:26,:,:]\n",
    "ch_dev_pps = ch_stacked[26:29,:,:]\n",
    "ch_test_pps = ch_stacked[29:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15207, 5, 200])\n",
      "torch.Size([1690, 5, 200])\n",
      "torch.Size([2816, 5, 200])\n",
      "torch.Size([15481, 5, 200])\n",
      "torch.Size([1787, 5, 200])\n",
      "torch.Size([2382, 5, 200])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "def pad(pp_data, time_steps=5):\n",
    "    pp_data = pp_data.reshape(np.prod(pp_data.shape[:-1]), pp_data.shape[-1])\n",
    "    vectors = [torch.tensor(i, dtype=torch.float32) for i in pp_data]\n",
    "    #print(len(vectors))\n",
    "    sequences = []\n",
    "    for i in range(int(len(vectors)/time_steps)+1):\n",
    "        if i * time_steps == len(vectors):\n",
    "            break\n",
    "        loc = i*5\n",
    "        sequences.append((vectors[loc:loc+time_steps]))\n",
    "    #print(len(sequences))\n",
    "    #print(sequences[0])\n",
    "    padded = pad_sequence([torch.stack(seq) for seq in sequences], batch_first=True, padding_value=0)\n",
    "    print(padded.shape)\n",
    "    return padded\n",
    "\n",
    "padded_en_train = pad(en_train_pps)\n",
    "padded_en_dev = pad(en_dev_pps)\n",
    "padded_en_test = pad(en_test_pps)\n",
    "padded_ch_train = pad(ch_train_pps)\n",
    "padded_ch_dev = pad(ch_dev_pps)\n",
    "padded_ch_test = pad(ch_test_pps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels\n",
    "# 0 = English; Chinese = 1\n",
    "en_train_labels = torch.zeros(padded_en_train.shape[0])\n",
    "en_dev_labels = torch.zeros(padded_en_dev.shape[0])\n",
    "en_test_labels = torch.zeros(padded_en_test.shape[0])\n",
    "\n",
    "ch_train_labels = torch.ones(padded_ch_train.shape[0])\n",
    "ch_dev_labels = torch.ones(padded_ch_dev.shape[0])\n",
    "ch_test_labels = torch.ones(padded_ch_test.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30688, 5, 200])\n",
      "torch.Size([3477, 5, 200])\n",
      "torch.Size([5198, 5, 200])\n",
      "torch.Size([30688])\n",
      "torch.Size([3477])\n",
      "torch.Size([5198])\n"
     ]
    }
   ],
   "source": [
    "# Combine CH and EN to create final train, test, and dev\n",
    "\n",
    "Xtrain = torch.cat((padded_en_train, padded_ch_train), dim=0)\n",
    "Xdev = torch.cat((padded_en_dev, padded_ch_dev), dim=0)\n",
    "Xtest = torch.cat((padded_en_test, padded_ch_test), dim=0)\n",
    "ytrain = torch.cat((en_train_labels, ch_train_labels))\n",
    "ydev = torch.cat((en_dev_labels, ch_dev_labels))\n",
    "ytest = torch.cat((en_test_labels, ch_test_labels))\n",
    "\n",
    "print(Xtrain.shape)\n",
    "print(Xdev.shape)\n",
    "print(Xtest.shape)\n",
    "print(ytrain.shape)\n",
    "print(ydev.shape)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle train, test, and dev\n",
    "train_ndxs = np.random.permutation(len(Xtrain))\n",
    "Xtrain = Xtrain[train_ndxs]\n",
    "ytrain = ytrain[train_ndxs]\n",
    "\n",
    "dev_ndxs = np.random.permutation(len(Xdev))\n",
    "Xdev = Xdev[dev_ndxs]\n",
    "ydev = ydev[dev_ndxs]\n",
    "\n",
    "test_ndxs = np.random.permutation(len(Xtest))\n",
    "Xtest = Xtest[test_ndxs]\n",
    "ytest = ytest[test_ndxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tensors to a directory\n",
    "dir = 'preprocessed_data/'\n",
    "\n",
    "torch.save(Xtrain, dir + 'Xtrain.pt')\n",
    "torch.save(ytrain, dir + 'ytrain.pt')\n",
    "torch.save(Xdev, dir + 'Xdev.pt')\n",
    "torch.save(ydev, dir + 'ydev.pt')\n",
    "torch.save(Xtest, dir + 'Xtest.pt')\n",
    "torch.save(ytest, dir + 'ytest.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
